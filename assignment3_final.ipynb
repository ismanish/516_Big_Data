{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": false,
    "editable": false,
    "id": "8xKUiifPp32x",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "03a3d471723919640c6afdf62f40975f",
     "grade": false,
     "grade_id": "cell-6b7fdfa930d77221",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# SIADS 516: Homework 3\n",
    "\n",
    "- **Dr. Chris Teplovs**, School of Information, University of Michigan\n",
    "- **Kris Steinhoff**, School of Information, University of Michigan\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "fbfc8c28efd166e58f0a7c342326c2be",
     "grade": false,
     "grade_id": "cell-c60fc9d05ab1488d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "This homework assignment builds on the Spark DataFrame material we covered in class.\n",
    "\n",
    "You will be using a compressed version of the Yelp Academic Dataset.  The data set is provided for you in the assets/data/yelp_academic of your workspace and you should not need to download it again if you're working on the Coursera hosted notebook environment.\n",
    "\n",
    "You might want to refer to the lecture companion notebooks (in resources/lecture_notebooks/ or equivalently via Coursera as \"Ungraded Lab: Spark Core Demo\" and \"Ungraded Lab: Spark SQL Demo) for hints about libraries to import, etc.\n",
    "\n",
    "You will notice that there are a **lot** of reviews.  You might want to work off a small sample (i.e. use the sample() function in Spark) to work on a reduced size dataset while you're developing your solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d9eaf180ea143f9255b5e744fc534936",
     "grade": false,
     "grade_id": "cell-58e2fd655ada3555",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# The AutograderHelper class provides methods used by the autograder.\n",
    "from autograder_helper import AutograderHelper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0477ef5614295bf8caf6e1096266bfa3",
     "grade": true,
     "grade_id": "inject_private_helper",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder cell. This cell is worth 0 points.\n",
    "# This cell has hidden code used to configure the autograder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5acc488dcb25bdbf68a5a61a683cb329",
     "grade": false,
     "grade_id": "cell-59a22c3dbff11ad2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/07/13 06:49:17 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession \n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName('My First Spark application') \\\n",
    "    .getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "09138434c9157bb972fb5311e3452059",
     "grade": false,
     "grade_id": "cell-2fe6f169887280fd",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Set up some RDDs:\n",
    "user = spark.read.json('../../assets/data/yelp_academic/yelp_academic_dataset_user.json.gz')\n",
    "review = spark.read.json('../../assets/data/yelp_academic/yelp_academic_dataset_review.json.gz')\n",
    "checkin = spark.read.json('../../assets/data/yelp_academic/yelp_academic_dataset_checkin.json.gz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1b5ce90448f11d395ec5cd997bf6e429",
     "grade": false,
     "grade_id": "cell-c4995d5242cd1768",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## -- COOL COMPLIMENTS --\n",
    "\n",
    "Determine how many users have received more than 5000 \"cool\" compliments.\n",
    "\n",
    "- Create a variable `user_count` (an integer) which contains the number of user with more than 5000 \"cool\" compliments (using the `compliment_cool` field.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- business_id: string (nullable = true)\n",
      " |-- cool: long (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      " |-- funny: long (nullable = true)\n",
      " |-- review_id: string (nullable = true)\n",
      " |-- stars: double (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      " |-- useful: long (nullable = true)\n",
      " |-- user_id: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "review.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a5c0e1e3c90fac25a0cdc87d430e9652",
     "grade": false,
     "grade_id": "cell-66bc2f2a925467c6",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "user_count = user.filter(user['compliment_cool'] > 5000).count()\n",
    "#raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a28f6e005f28828aa341bc6552ef3077",
     "grade": false,
     "grade_id": "cell-123aa59e04fda184",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert type(user_count) == int, \"The user_count variable should be an integer.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7a301b60a72d2b0c5989036e763ed67e",
     "grade": true,
     "grade_id": "cell-167a913f1dc689ce",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder cell. This cell is worth 2 points (out of 20). This cell contains hidden tests."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "29f4bcd3be69bb93084cf273b04f7de6",
     "grade": false,
     "grade_id": "cell-8044ac07d496501d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## -- USEFUL POSITIVE REVIEWS --\n",
    "\n",
    "Determine the top 5 most useful positive reviews.\n",
    "\n",
    "- Create a variable `top_5_useful_positive`. This should be a PySpark DataFrame\n",
    "- For this question a \"positive review\" is one with 4 or 5 stars\n",
    "- The DataFrame should be ordered by `useful` and contain 5 rows\n",
    "- The DataFrame should have these columns:\n",
    "    - `review_id`\n",
    "    - `useful`\n",
    "    - `stars`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cd33677345bfe6594a9ab1cc4f868e84",
     "grade": false,
     "grade_id": "cell-54c8c4e7596531e4",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "top_5_useful_positive = review.filter((review['stars'] ==4) | (review['stars'] ==5)).\\\n",
    "sort(\"useful\", ascending = False)[['review_id','useful','stars']].limit(5)\n",
    "\n",
    "#raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "81f2c69737da9d2deb2acdcc3c2964e4",
     "grade": false,
     "grade_id": "cell-0b3e82dd685d24d2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "\n",
    "assert type(top_5_useful_positive) == pyspark.sql.dataframe.DataFrame, \\\n",
    "    \"The top_useful_positive variable should be a Spark DataFrame.\"\n",
    "\n",
    "submitted = AutograderHelper.parse_spark_dataframe(top_5_useful_positive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "465146a4e784cd70917bb9b3b6fcf3d4",
     "grade": true,
     "grade_id": "cell-af0f871a7824a9cd",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder cell. This cell is worth 1 point (out of 20). This cell does not contain hidden tests.\n",
    "# This cell deliberately includes answers to provide guidance on how this question is graded.\n",
    "\n",
    "assert len(submitted) == 5, \\\n",
    "    \"The result must have 5 rows.\"\n",
    "\n",
    "top_useful_review_id = \"1lGXlyq4MALOMx17vpBcoQ\"\n",
    "assert submitted[\"review_id\"][0] == top_useful_review_id, \\\n",
    "    f'The first row should have review_id \"{top_useful_review_id}\" (this review has the most \"useful\" ratings)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "34b82eb96d4e50545169a20166fc0fbd",
     "grade": true,
     "grade_id": "cell-e95ff3117c1fb4df",
     "locked": true,
     "points": 4,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder cell. This cell is worth 4 points (out of 20). This cell contains hidden tests."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0c81e192adce8d5297300371d1d4d90a",
     "grade": false,
     "grade_id": "cell-89161ab4899d135c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## -- CHECKINS --\n",
    "\n",
    "Determine what hours of the day most checkins occur.\n",
    "\n",
    "- Create a variable `hours_by_checkin_count`. This should be a PySpark DataFrame\n",
    "- The DataFrame should be ordered by `count` and contain 24 rows\n",
    "- The DataFrame should have these columns:\n",
    "    - `hour` (the hour of the day as an integer, 0-23)\n",
    "    - `count` (the number of checkins that occurred in that hour)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8f89e2e8748c0cf5d6287d9de48d6589",
     "grade": false,
     "grade_id": "cell-a2865bc601cc4990",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "\n",
    "from pyspark.sql.functions import udf, explode\n",
    "from pyspark.sql.types import IntegerType, StringType, ArrayType\n",
    "\n",
    "def hrs(x):\n",
    "    return [dt.split(\" \")[-1][:2] for dt in x.split(\",\")]\n",
    "\n",
    "hr_int = udf(lambda z: hrs(z), ArrayType(StringType()))\n",
    "\n",
    "df = checkin.select('business_id', hr_int('date').alias('hours'))\n",
    "\n",
    "df = df.withColumn('hour', explode('hours'))\n",
    "df = df.withColumn(\"hour\", df[\"hour\"].cast(IntegerType()))\n",
    "\n",
    "hours_by_checkin_count = df.groupby(['hour']).count().sort('count', ascending = False)\n",
    "# NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "340a16186b9323916455872bbf8889d5",
     "grade": false,
     "grade_id": "cell-51014951b91565ce",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "assert type(hours_by_checkin_count) == pyspark.sql.dataframe.DataFrame, \\\n",
    "    \"The top_useful_positive variable should be a Spark DataFrame.\"\n",
    "\n",
    "submitted = AutograderHelper.parse_spark_dataframe(hours_by_checkin_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ab93f95c981e68fb61af722dbbd86c15",
     "grade": true,
     "grade_id": "cell-6c0f04ddb2c1d376",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder cell. This cell is worth 1 point (out of 20). This cell does not contain hidden tests.\n",
    "\n",
    "assert len(submitted) == 24, \\\n",
    "    \"The top_useful_positive DataFrame must have 24 rows.\"\n",
    "\n",
    "assert submitted[\"hour\"][0] == 1, \\\n",
    "    'The first row should have hour 1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5646b8412015029c5a3fe2045cd30479",
     "grade": true,
     "grade_id": "cell-86eec1c279bef331",
     "locked": true,
     "points": 4,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder cell. This cell is worth 4 points (out of 20). This cell contains hidden tests."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8ea483fd0b2cb398feba1584f7203740",
     "grade": false,
     "grade_id": "cell-f34ec996454b8d30",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## -- COMMON USEFUL WORDS --\n",
    "\n",
    "Write a function that takes a Spark DataFrame as a parameter and returns a Spark DataFrame of the 50 most common words from *useful* reviews and their counts.\n",
    "\n",
    "- A \"useful review\" has 10 or more \"useful\" ratings.\n",
    "- Convert the text to lower case.\n",
    "- Use the provided `splitter()` function in a UDF to split the text into individual words.\n",
    "- Exclude the words in the provided `STOP_WORDS` set.\n",
    "- Returned DataFrame should have these columns:\n",
    "    - `word`\n",
    "    - `count`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e5e30222ef480b049506fed150c112d3",
     "grade": false,
     "grade_id": "cell-553c4a0f96a66555",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def splitter(text):\n",
    "    WORD_RE = re.compile(r\"[\\w']+\")\n",
    "    return WORD_RE.findall(text)\n",
    "\n",
    "\n",
    "STOP_WORDS = {\n",
    "    \"a\", \"about\", \"above\", \"after\", \"again\", \"against\", \"aint\", \"all\", \"also\", \"although\", \"am\", \"an\", \"and\", \"any\",\n",
    "    \"are\", \"as\", \"at\", \"be\", \"because\", \"been\", \"before\", \"being\", \"below\", \"between\", \"both\", \"but\", \"by\", \"can\",\n",
    "    \"check\", \"checked\", \"could\", \"did\", \"do\", \"does\", \"doing\", \"don\", \"down\", \"during\", \"each\", \"few\", \"for\", \"from\",\n",
    "    \"further\", \"get\", \"go\", \"got\", \"had\", \"has\", \"have\", \"having\", \"he\", \"her\", \"here\", \"hers\", \"herself\", \"him\",\n",
    "    \"himself\", \"his\", \"how\", \"however\", \"i\", \"i'd\", \"if\", \"i'm\", \"in\", \"into\", \"is\", \"it\", \"its\", \"it's\", \"itself\",\n",
    "    \"i've\", \"just\", \"me\", \"more\", \"most\", \"my\", \"myself\", \"no\", \"nor\", \"not\", \"now\", \"of\", \"off\", \"on\", \"once\", \"one\",\n",
    "    \"online\", \"only\", \"or\", \"other\", \"our\", \"ours\", \"ourselves\", \"out\", \"over\", \"own\", \"paid\", \"place\", \"s\", \"said\", \n",
    "    \"same\", \"service\", \"she\", \"should\", \"so\", \"some\", \"such\", \"t\", \"than\", \"that\", \"the\", \"their\", \"theirs\", \"them\",\n",
    "    \"themselves\", \"then\", \"there\", \"these\", \"they\", \"this\", \"those\", \"through\", \"to\", \"too\", \"under\", \"until\", \"up\",\n",
    "    \"us\", \"very\", \"was\", \"we\", \"went\", \"were\", \"we've\", \"what\", \"when\", \"where\", \"which\", \"while\", \"who\", \"whom\",\n",
    "    \"why\", \"will\", \"with\", \"would\", \"you\", \"your\", \"yours\", \"yourself\", \"yourselves\",\n",
    "}\n",
    "\n",
    "def common_useful_words(reviews, limit=50):\n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    import pyspark.sql.functions as f\n",
    "    from pyspark.sql.functions import lower, col\n",
    "\n",
    "    df = review.filter(review['useful']>=10)\n",
    "\n",
    "    df = df.withColumn('text', lower(col('text')))\n",
    "\n",
    "    df = df.select(\"text\")\n",
    "\n",
    "    split_fn = udf(lambda z: splitter(z), ArrayType(StringType()))\n",
    "\n",
    "    df = df.select(split_fn('text').alias('text'))\n",
    "    df = df.withColumn('word', explode('text'))\n",
    "    df = df.filter(~df['word'].isin(STOP_WORDS))\n",
    "\n",
    "\n",
    "    most_common = df.groupby(['word']).count().sort('count', ascending = False).limit(limit)    \n",
    "    \n",
    "    #raise NotImplementedError()\n",
    "    \n",
    "    return most_common"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2a11b6eaf8c132f879f017240b47472a",
     "grade": false,
     "grade_id": "cell-b0171a78011d3ed9",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Now we'll run it on the `review` DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pyspark.sql.functions as f\n",
    "# from pyspark.sql.functions import lower, col\n",
    "\n",
    "# df = review.filter(review['useful']>=10)\n",
    "\n",
    "# df = df.withColumn('text', lower(col('text')))\n",
    "\n",
    "# df = df.select(\"text\")\n",
    "\n",
    "# split_fn = udf(lambda z: splitter(z), ArrayType(StringType()))\n",
    "\n",
    "# df = df.select(split_fn('text').alias('text'))\n",
    "# df = df.withColumn('word', explode('text'))\n",
    "# df = df.filter(~df['word'].isin(STOP_WORDS))\n",
    "\n",
    "\n",
    "# df = df.groupby(['word']).count().sort('count', ascending = False)\n",
    "\n",
    "\n",
    "# df = df.limit(50)\n",
    "\n",
    "# df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b4c249aec91877f6af2c6f33f2d6901e",
     "grade": false,
     "grade_id": "cell-f43fd0bde77106de",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "common_useful_words_counts = common_useful_words(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "66f2dfe045e0f528ec2fe93c13c3781d",
     "grade": false,
     "grade_id": "cell-d1aa94412d93396f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "assert type(common_useful_words_counts) == pyspark.sql.dataframe.DataFrame, \\\n",
    "    \"The common_useful_words_counts variable should be a Spark DataFrame.\"\n",
    "\n",
    "submitted = AutograderHelper.parse_spark_dataframe(common_useful_words_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e157eb7d83e6822785a86da7b14ccd0f",
     "grade": true,
     "grade_id": "cell-9c3ca29528e327be",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder cell. This cell is worth 2 points (out of 20). This cell does not contain hidden tests.\n",
    "\n",
    "assert len(submitted) == 50, \\\n",
    "    \"The common_useful_words_counts DataFrame must have 24 rows.\"\n",
    "\n",
    "assert submitted[\"word\"][0] == 'like', \\\n",
    "    'The first row should have word \"like\"'\n",
    "\n",
    "assert submitted[\"count\"][0] == 101251, \\\n",
    "    'The first row should have count 101251'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0fe10c14a82026c86ac9bf8743dea9fe",
     "grade": true,
     "grade_id": "cell-d2a7766721f0da5b",
     "locked": true,
     "points": 6,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder cell. This cell is worth 6 points (out of 20). This cell contains hidden tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "coursera": {
   "schema_names": [
    "mads_big_data_scalable_data_processing_v3_assignment3"
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
