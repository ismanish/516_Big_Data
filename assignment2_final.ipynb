{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a4cca1d8db784dd2c613b31d033f8542",
     "grade": false,
     "grade_id": "cell-57c834a1f738c74d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# SIADS 516: Homework 2\n",
    "\n",
    "- **Dr. Chris Teplovs**, School of Information, University of Michigan\n",
    "- **Kris Steinhoff**, School of Information, University of Michigan\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d9eaf180ea143f9255b5e744fc534936",
     "grade": false,
     "grade_id": "cell-58e2fd655ada3555",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# The AutograderHelper class provides methods used by the autograder.\n",
    "from autograder_helper import AutograderHelper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0477ef5614295bf8caf6e1096266bfa3",
     "grade": true,
     "grade_id": "inject_private_helper",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder cell. This cell is worth 0 points.\n",
    "# This cell has hidden code used to configure the autograder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "925c064911f69aae23bada5c2700181f",
     "grade": false,
     "grade_id": "cell-616a4489bd782048",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Using the Spark RDD API to analyze text\n",
    "Data are from \n",
    "https://www.kaggle.com/nzalake52/new-york-times-articles\n",
    "\n",
    "## Objectives\n",
    "1. To gain familiarity with PySpark\n",
    "2. To learn the basics of the Spark RDD API\n",
    "3. To practice solving a real-world problem\n",
    "\n",
    "## Overview\n",
    "\n",
    "This project was inspired by an actual event that was experienced by a UMSI student.  This student was applying for a \n",
    "job with a large multi-national corporation (let's call it XYZ, Inc.).  XYZ Inc. was looking for someone who could \n",
    "conduct an analysis of a massive (terabyte-size) text dataset.  They had heard about Spark and planned on investigating it but hadn't yet found someone internally who had the skill set required to tackle the problem.  The UMSI student indicated that they had experience with Spark and could likely handle the task.  The hiring supervisor then provided a non-Spark script and asked the student to demonstrate how that script could be translated to work in a Spark environment.  The student was able to do the conversion and, pending completion of their degree, will have secured a job at XYZ, Inc.\n",
    "\n",
    "This assignment simulates that exact situation.  **In this assignment you will take a python-based script that does\n",
    "part-of-speech tagging on a large dataset and convert it, as much as possible, to use a pyspark-based approach.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6a7beb99d8118f9247b6e6a1e97b1f38",
     "grade": false,
     "grade_id": "cell-ed5926af9a297155",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "---\n",
    "\n",
    "### Task: Review non-Spark code\n",
    "\n",
    "The original script was written by Luke Petschauer and a forked version is available in this notebook: [NP_chunking_with_the_NLTK.ipynb](https://github.com/umsi-data-science/NP_chunking_with_nltk/blob/master/NP_chunking_with_the_NLTK.ipynb).\n",
    "\n",
    "It provides a detailed explanation of the original code and an excellent overview and justification for the use of\n",
    "part-of-speech tagging and a super-gentle introduction to Natural Language Processing (NLP).\n",
    "\n",
    "Let's use some of the code from that notebook here...\n",
    "\n",
    "We'll start by importing the required packages, and making sure the NLTK collections are downloaded to your environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e3d5d24c9dd8cae72407d4aaac8b34c3",
     "grade": false,
     "grade_id": "cell-69e163c2a163c158",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'book'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package abc to /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Package abc is already up-to-date!\n",
      "[nltk_data]    | Downloading package brown to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Package brown is already up-to-date!\n",
      "[nltk_data]    | Downloading package chat80 to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Package chat80 is already up-to-date!\n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Package cmudict is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2000 to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Package conll2000 is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2002 to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Package conll2002 is already up-to-date!\n",
      "[nltk_data]    | Downloading package dependency_treebank to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Package dependency_treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Package genesis is already up-to-date!\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
      "[nltk_data]    | Downloading package ieer to /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Package ieer is already up-to-date!\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Package inaugural is already up-to-date!\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
      "[nltk_data]    | Downloading package nps_chat to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Package nps_chat is already up-to-date!\n",
      "[nltk_data]    | Downloading package names to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Package names is already up-to-date!\n",
      "[nltk_data]    | Downloading package ppattach to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Package ppattach is already up-to-date!\n",
      "[nltk_data]    | Downloading package reuters to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Package reuters is already up-to-date!\n",
      "[nltk_data]    | Downloading package senseval to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Package senseval is already up-to-date!\n",
      "[nltk_data]    | Downloading package state_union to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Package state_union is already up-to-date!\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Package stopwords is already up-to-date!\n",
      "[nltk_data]    | Downloading package swadesh to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Package swadesh is already up-to-date!\n",
      "[nltk_data]    | Downloading package timit to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Package timit is already up-to-date!\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Package treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package toolbox to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Package toolbox is already up-to-date!\n",
      "[nltk_data]    | Downloading package udhr to /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Package udhr is already up-to-date!\n",
      "[nltk_data]    | Downloading package udhr2 to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Package udhr2 is already up-to-date!\n",
      "[nltk_data]    | Downloading package unicode_samples to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Package unicode_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package webtext to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Package webtext is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Package wordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
      "[nltk_data]    | Downloading package words to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Package words is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Package maxent_treebank_pos_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data]    | Downloading package universal_tagset to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Package universal_tagset is already up-to-date!\n",
      "[nltk_data]    | Downloading package punkt to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Package punkt is already up-to-date!\n",
      "[nltk_data]    | Downloading package book_grammars to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Package book_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package city_database to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Package city_database is already up-to-date!\n",
      "[nltk_data]    | Downloading package tagsets to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Package tagsets is already up-to-date!\n",
      "[nltk_data]    | Downloading package panlex_swadesh to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Package panlex_swadesh is already up-to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection book\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import re\n",
    "import pprint\n",
    "from nltk import Tree\n",
    "\n",
    "nltk.download('book') # NOTE: this should be unnecessary for Coursera image (should be preloaded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "cda370f5d2f870e04e80ee377a582354",
     "grade": false,
     "grade_id": "cell-4b6d494379749e78",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "The code in the next cell is from the \"Final Code\" section in the NP_chunking_with_the_NLTK.ipynb notebook (linked above). This implementation doesn't use Spark. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cc8897276308fed9f313eacba9e86500",
     "grade": false,
     "grade_id": "cell-0497b0506c825bdc",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# This is the original (non-Spark) script\n",
    "\n",
    "patterns = \"\"\"\n",
    "    NP: {<JJ>*<NN*>+}\n",
    "    {<JJ>*<NN*><CC>*<NN*>+}\n",
    "    \"\"\"\n",
    "\n",
    "NPChunker = nltk.RegexpParser(patterns)\n",
    "\n",
    "def prepare_text(input):\n",
    "    sentences = nltk.sent_tokenize(input)\n",
    "    sentences = [nltk.word_tokenize(sent) for sent in sentences]\n",
    "    sentences = [nltk.pos_tag(sent) for sent in sentences]\n",
    "    sentences = [NPChunker.parse(sent) for sent in sentences]\n",
    "    return sentences\n",
    "\n",
    "\n",
    "def parsed_text_to_NP(sentences):\n",
    "    nps = []\n",
    "    for sent in sentences:\n",
    "        tree = NPChunker.parse(sent)\n",
    "        for subtree in tree.subtrees():\n",
    "            if subtree.label() == 'NP':\n",
    "                t = subtree\n",
    "                t = ' '.join(word for word, tag in t.leaves())\n",
    "                nps.append(t)\n",
    "    return nps\n",
    "\n",
    "\n",
    "def sent_parse(input):\n",
    "    sentences = prepare_text(str(input))\n",
    "    nps = parsed_text_to_NP(sentences)\n",
    "    return nps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c9e940b588c387068b054e75226a639b",
     "grade": false,
     "grade_id": "cell-9c0294caf65ac26a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Stellar pitching', 'afloat', 'first half', 'last season', 'encore', 'pennant-winning season', 'lineup', 'pitching', 'thin', 'pitching', 's game', 'pitching', 'anything', '4-2 loss', 'place', 'spot starter', 'deficit', 'lineup', 'starter', 'room', 'ninth inning', 'last-gasp two-run homer', 'reliever', 'streak', 'team']\n"
     ]
    }
   ],
   "source": [
    "text_to_be_analyzed = \"\"\"\\\n",
    "WASHINGTON - Stellar pitching kept the Mets afloat in the first half of last season despite their offensive \n",
    "woes. But they cannot produce an encore of their pennant-winning season if their lineup keeps floundering \n",
    "while their pitching is nicked, bruised and stretched thin.\n",
    "\n",
    "\"We were going to ride our pitching,\" Manager Terry Collins said before Wednesday’s game. \"But we're not \n",
    "riding it right now. We've got as many problems with our pitching as we do anything.\"\n",
    "\n",
    "Wednesday's 4-2 loss to the Washington Nationals was cruel for the already-limping Mets. Pitching in Steven \n",
    "Matz's place, the spot starter Logan Verrett allowed two runs over five innings. But even that was too large \n",
    "a deficit for the Mets' lineup to overcome against Max Scherzer, the Nationals' starter.\n",
    "\n",
    "\"We're not even giving ourselves chances,\" Collins said, adding later, \"We just can’t give our pitchers any \n",
    "room to work.\"\n",
    "\n",
    "The Mets did not score until the ninth inning, when a last-gasp two-run homer by James Loney off Nationals \n",
    "reliever Shawn Kelley snapped a streak of 23 scoreless innings for the team.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "nps = sent_parse(text_to_be_analyzed)\n",
    "\n",
    "# Print a list of noun phrases found in text_to_be_analyzed\n",
    "print(nps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "45d29c9213b498bcbdb9302245d16620",
     "grade": false,
     "grade_id": "cell-2b1b8b07150d6206",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "You will be taking a similar approach to analyze a large set of news articles from the New York Times using pyspark.\n",
    "\n",
    "**Before you continue to the next task, you should:**\n",
    "- read through and study the NP_chunking_with_the_NLTK.ipynb notebook (linked above)\n",
    "- study and run the cells above.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5a1b9e65927a24cb8d87890dff88ae25",
     "grade": false,
     "grade_id": "cell-843d96c7f5283aef",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/07/08 07:02:45 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "# Import packages and set up a Spark Session and Context.\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName('SIADS 516 Homework 2') \\\n",
    "    .getOrCreate() \n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a9a253f29241cb7e917f1b3da8f71c84",
     "grade": false,
     "grade_id": "cell-5277c09229a54ff0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "---\n",
    "\n",
    "## -- PART-OF-SPEECH COUNT --\n",
    "\n",
    "### Task: Create an RDD pipline to show the count of each part-of-speach tag sorted in descending order\n",
    "\n",
    "Complete the implementation of the `pos_counts()` function below so that it uses an RDD pipeline (i.e. sequence of transformations) to:\n",
    "1. filter out blank lines \n",
    "2. filter out lines starting with 'URL'\n",
    "3. create a single list (using flatMap) that applies the `pos_tag_counter()` (this is defined below for you below) function to each line\n",
    "4. map each resulting line to show the part of speech (which is the second element returned from the pos_tag_counter)\n",
    "5. convert each resulting line to a pairRDD with POS tags as keys and values of 1\n",
    "6. reduce the resulting RDD by key, adding up all the 1s (like the lecture and lab examples)\n",
    "7. sort the resulting list by the counts, in descending order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f6b6a7bb9b09470daf5aecbca0305ba0",
     "grade": false,
     "grade_id": "cell-b2108786df8cf65a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# This is the function you will use with flatMap in your pipeline.\n",
    "\n",
    "TOKEN_RE = re.compile(r\"\\b[\\w']+\\b\")\n",
    "def pos_tag_counter(line):\n",
    "    toks = nltk.regexp_tokenize(line, TOKEN_RE)\n",
    "    postoks = nltk.tag.pos_tag(toks)\n",
    "    return postoks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "44cf54ebab99b7299290594f541294f5",
     "grade": false,
     "grade_id": "cell-9423e057bc9888ca",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def pos_counts(rdd):\n",
    "    \n",
    "\n",
    "    rdd = rdd.filter(lambda x: len(x) > 0)\n",
    "    rdd = rdd.filter(lambda x: str(x)[:3]!= \"URL\")\n",
    "    pos_total_sorted = rdd.flatMap(lambda x: pos_tag_counter(x))\n",
    "    pos_total_sorted = pos_total_sorted.map(lambda x: (x[-1],1))\n",
    "    pos_total_sorted=pos_total_sorted.reduceByKey(lambda a,b: a+b)\n",
    "    pos_total_sorted = pos_total_sorted.sortBy(lambda x: -x[1])\n",
    "    \n",
    "\n",
    "    \n",
    "    return pos_total_sorted  # This should be the final stage of your pipeline, an RDD with the \n",
    "                             # count of each part-of-speach tag sorted in descending order."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "44bd187bccf286ab8a63358789dacd05",
     "grade": false,
     "grade_id": "cell-f6c557b3ad5654af",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Let's start by trying your code on a small data set. The `text_to_be_analyzed` from the cells above will do nicely. We can use the parallelize() method to turn it into a RDD, pass that to your function, and then take() the first ten entries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ab31f5f7e8cf0e4470a88f205a85ab35",
     "grade": false,
     "grade_id": "cell-b7eab46c73659587",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "small_text = sc.parallelize(text_to_be_analyzed.split(\"\\n\"))\n",
    "\n",
    "small_pos_counts = pos_counts(small_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "26da3475b67021a47574d4e58693f415",
     "grade": false,
     "grade_id": "cell-32f7fcf64cf8c16c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('NN', 30),\n",
       " ('NNP', 24),\n",
       " ('IN', 20),\n",
       " ('DT', 16),\n",
       " ('VBD', 11),\n",
       " ('RB', 11),\n",
       " ('JJ', 11),\n",
       " ('NNS', 10),\n",
       " ('PRP$', 7),\n",
       " ('VB', 7)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "small_pos_counts_take_10 = small_pos_counts.take(10)\n",
    "small_pos_counts_take_10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "67917b354291a4ff3d07e5b34555dcd4",
     "grade": true,
     "grade_id": "cell-78a1082bd066de62",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder cell. This cell is worth 2 points (out of 20). This cell does not contain hidden tests.\n",
    "# This cell deliberately includes answers to provide guidance on how this question is graded.\n",
    "\n",
    "correct = AutograderHelper.parse_spark_take([\n",
    "    ('NN', 30),\n",
    "    ('NNP', 24),\n",
    "    ('IN', 20),\n",
    "    ('DT', 16),\n",
    "    ('VBD', 11),\n",
    "    ('JJ', 11),\n",
    "    ('RB', 11),\n",
    "    ('NNS', 10),\n",
    "    ('PRP$', 7),\n",
    "    ('VB', 7),\n",
    "])\n",
    "\n",
    "AutograderHelper.assert_same_shape(\n",
    "    correct=correct,\n",
    "    submitted=AutograderHelper.parse_spark_take(small_pos_counts_take_10),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c776a4b67ba72493f1faab877eca8c2c",
     "grade": false,
     "grade_id": "cell-81cfccd875008e50",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Now let's run it against a much larger data set. *The complete analysis could take about 10 minutes to run.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4692d4b01a549e0c4d9f2a519931a91a",
     "grade": false,
     "grade_id": "cell-eb242df1b65fdff7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "text = sc.textFile('../../assets/data/nytimes/nytimes_news_articles.txt')\n",
    "\n",
    "pos_counts = pos_counts(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5f4e2aa488ae7e8a773e2760fe995781",
     "grade": false,
     "grade_id": "cell-7940abf4b4dfa9aa",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('NN', 1126515),\n",
       " ('IN', 928916),\n",
       " ('NNP', 853093),\n",
       " ('DT', 761492),\n",
       " ('JJ', 498482),\n",
       " ('NNS', 437116),\n",
       " ('VBD', 379509),\n",
       " ('PRP', 282603),\n",
       " ('RB', 271053),\n",
       " ('CC', 231491)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_counts_take_10 = pos_counts.take(10)\n",
    "pos_counts_take_10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6aa687520f7adf81bc37478c1b65d70a",
     "grade": true,
     "grade_id": "cell-a97e03c0110ec601",
     "locked": true,
     "points": 8,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder cell. This cell is worth 8 points (out of 20). This cell contains hidden tests."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "40c8017b188ded8cd939e5314e58d510",
     "grade": false,
     "grade_id": "cell-c252601f5d0f6eae",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "---\n",
    "\n",
    "## -- NOUN PHRASE LENGTH --\n",
    "\n",
    "### Task: Create an RDD pipeline to show the distribution of the length of noun phrases\n",
    "\n",
    "Complete the implementation of the `noun_phrase_length_distribution()` function below so that it uses an RDD pipeline  to return a PairRDD which contains the distribution of the length of noun phrases.\n",
    "\n",
    "- You can apply the `tokenize_chunk_parse()` (defined below for you) to apply--with flatMap()--to each entry in the input RDD.\n",
    "- Sorting the resulting list by the counts in descending order will make the results easier to interpret."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "eb9975656c2c000c274f1583d0f85140",
     "grade": false,
     "grade_id": "cell-2be2c016bf978c20",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# This cell defines the tokenize_chunk_parse() function you will use with flatMap()\n",
    "\n",
    "grammar = r\"\"\"\n",
    "    NBAR:\n",
    "        {<NN.*|JJS>*<NN.*>}\n",
    "        \n",
    "    NP:\n",
    "        {<NBAR>}\n",
    "        {<NBAR><IN><NBAR>}\n",
    "\"\"\"\n",
    "\n",
    "  \n",
    "def tokenize_chunk_parse(line):\n",
    "    chunker = nltk.RegexpParser(grammar)\n",
    "  \n",
    "    toks = nltk.regexp_tokenize(line, TOKEN_RE)\n",
    "    postoks = nltk.tag.pos_tag(toks)\n",
    "\n",
    "    if len(postoks) == 0:\n",
    "        return []\n",
    "    \n",
    "    tree = chunker.parse(postoks)\n",
    "\n",
    "    return [term for term in leaves(tree)] \n",
    "  \n",
    "def leaves(tree):\n",
    "    for subtree in tree.subtrees(filter = lambda t: t.label()=='NP'):\n",
    "        yield subtree.leaves()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "04f0cbb24665e4525baff71b8b834ad8",
     "grade": false,
     "grade_id": "cell-7b9935797739cfb4",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def noun_phrase_length_distribution(rdd):\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    #raise NotImplementedError()\n",
    "    distribution = rdd.flatMap(lambda x: tokenize_chunk_parse(x))\n",
    "    #distribution = rdd.flatMap(lambda x: leaves(distribution))\n",
    "    #distribution = distribution.map(lambda x: (x[-1],1))\n",
    "    distribution = distribution.map(lambda x: (len(x),1))\n",
    "    distribution=distribution.reduceByKey(lambda a,b: a+b)\n",
    "    distribution = distribution.sortBy(lambda x: -1*x[1])\n",
    "\n",
    "    return distribution  # This should be the final stage of your pipeline, a PairRDD with the \n",
    "                         # distribution of the length of noun phrases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7718d888079b87f58a957677005090e1",
     "grade": false,
     "grade_id": "cell-598658c1ef153bda",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "small_counts = noun_phrase_length_distribution(small_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "12e4b1fc8116015e033438cfdf3f503a",
     "grade": false,
     "grade_id": "cell-b86490c94c92960c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 29), (2, 10), (3, 3), (4, 2)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "small_counts_take_10 = small_counts.take(10)\n",
    "small_counts_take_10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a6b737c3a9477527a6b9976e24677636",
     "grade": false,
     "grade_id": "cell-1274d338a9180567",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "The cell above should produce this output:\n",
    "\n",
    "```\n",
    "[(1, 29), (2, 10), (3, 3), (4, 2)]\n",
    "```\n",
    "\n",
    "This means there are 29 1-word noun phrases, 10 2-word noun phrases, 3 3-word noun phrases, and 2 4-word noun phrases in the `small_text` data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4cffd3f0a5803d68f07f0026624f9cf9",
     "grade": true,
     "grade_id": "cell-aacaf880a29e2388",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder cell. This cell is worth 2 points (out of 20). This cell does not contain hidden tests.\n",
    "# This cell deliberately includes answers to provide guidance on how this question is graded.\n",
    "\n",
    "correct = AutograderHelper.parse_spark_take(\n",
    "    [(1, 29), (2, 10), (3, 3), (4, 2)]\n",
    ")\n",
    "\n",
    "AutograderHelper.assert_same_shape(\n",
    "    correct=correct,\n",
    "    submitted=AutograderHelper.parse_spark_take(small_counts_take_10),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9beeba4e3ecf0a630a35c183b0f5e7ca",
     "grade": false,
     "grade_id": "cell-3b0484807ae14698",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Now let's run it against the larger data set. *The complete analysis could take about 10 minutes to run.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f5f41fa25eecacbec2b20c45b3240696",
     "grade": false,
     "grade_id": "cell-6cde15c3035b573e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "text = sc.textFile('../../assets/data/nytimes/nytimes_news_articles.txt')\n",
    "\n",
    "counts = noun_phrase_length_distribution(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "13f563900e7d2b6e4bad861cd32f8da4",
     "grade": false,
     "grade_id": "cell-ec207bb87cf9a98e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 1205976),\n",
       " (2, 353457),\n",
       " (3, 119065),\n",
       " (4, 35890),\n",
       " (5, 11079),\n",
       " (6, 3889),\n",
       " (7, 1400),\n",
       " (8, 543),\n",
       " (9, 257),\n",
       " (10, 112)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts_take_10 = counts.take(10)\n",
    "counts_take_10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ada664af369f03250519c71d41398234",
     "grade": true,
     "grade_id": "cell-b332beabf52236e3",
     "locked": true,
     "points": 8,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder cell. This cell is worth 8 points (out of 20). This cell contains hidden tests."
   ]
  }
 ],
 "metadata": {
  "coursera": {
   "schema_names": [
    "mads_big_data_scalable_data_processing_v3_assignment2"
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "name": "si330wn18hw9",
  "notebookId": 258872427021699
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
